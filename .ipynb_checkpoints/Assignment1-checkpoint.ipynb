{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 1: Decision Trees\n",
    "\n",
    "In this assignment we'll implement the Decision Tree algorithm to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the Diabetic Retinopathy data set, which contains features from the Messidor image set to predict whether an image contains signs of diabetic retinopathy or not. This dataset has `1151` instances and `20` attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "0) The binary result of quality assessment. 0 = bad quality 1 = sufficient quality.\n",
    "\n",
    "1) The binary result of pre-screening, where 1 indicates severe retinal abnormality and 0 its lack. \n",
    "\n",
    "2-7) The results of MA detection. Each feature value stand for the number of MAs found at the confidence levels alpha = 0.5, . . . , 1, respectively. \n",
    "\n",
    "8-15) contain the same information as 2-7) for exudates. However, as exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the \n",
    "number of lesions with the diameter of the ROI to compensate different image sizes. \n",
    "\n",
    "16) The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "\n",
    "17) The diameter of the optic disc. \n",
    "\n",
    "18) The binary result of the AM/FM-based classification.\n",
    "\n",
    "19) Class label. 1 = contains signs of Diabetic Retinopathy (Accumulative label for the Messidor classes 1, 2, 3), 0 = no signs of Diabetic Retinopathy.\n",
    "\n",
    "\n",
    "A few function prototypes are already given to you, please don't change those. You can add additional helper functions for your convenience. *Suggestion:* The dataset is substantially big, for the purpose of easy debugging work with a subset of the data and test your decision tree implementation on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: \n",
    "A few function prototypes are already given to you, please don't change those. You can add additional helper functions for your convenience. \n",
    "\n",
    "*Suggestion:* The dataset is substantially big, for the purpose of easy debugging, work with a subset of the data and test your decision tree implementation on that.\n",
    "\n",
    "#### Notes:\n",
    "Parts of this assignment will be **autograded** so a couple of caveats :-\n",
    "- Entropy is calculated using log with base 2, `math.log2(x)`.\n",
    "- For continuous features ensure that the threshold value lies exactly between 2 buckets. For example, if for feature 2 the best split occurs between 10 and 15 then the threshold value will be set as 12.5.\n",
    "- For binary features [0/1] the threshold value will be 0.5. All values < `thresh_val` go to the left child and all values >= `thresh_val` go to the right child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "from math import floor\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    file_data = pd.read_csv(filename, skipinitialspace=True, header=None)\n",
    "    for i in range(0,len(file_data)):\n",
    "        data.append(DataPoint(file_data.iloc[i,19],file_data.iloc[i,0:19]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self):    # for debugging purposes\n",
    "        if self.is_leaf:\n",
    "            print ('Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree()\n",
    "            self.right_child.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tree_root, data_point):\n",
    "    while(not tree_root.is_leaf):\n",
    "        threshold = tree_root.thresh_val\n",
    "        feature_index = tree_root.feature_idx\n",
    "        if data_point.features[feature_index] < threshold:\n",
    "            tree_root = tree_root.left_child\n",
    "        else:\n",
    "            tree_root = tree_root.right_child\n",
    "    return tree_root.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    for dataPoint in data:\n",
    "        if dataPoint.features[feature_idx] < threshold:\n",
    "            left_split.append(dataPoint)\n",
    "        else:\n",
    "            right_split.append(dataPoint)\n",
    "    return (left_split, right_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    entropy = 0.0\n",
    "    noLabel = 0\n",
    "    yesLabel = 0\n",
    "    sizeData = len(data)\n",
    "    if sizeData == 0:\n",
    "        return 0\n",
    "    for dataPoint in data:\n",
    "        if dataPoint.label == 0:\n",
    "            noLabel = noLabel + 1\n",
    "        else:\n",
    "            yesLabel = yesLabel + 1\n",
    "    yesLabel = yesLabel/sizeData\n",
    "    noLabel = noLabel/sizeData\n",
    "    if yesLabel != 0:\n",
    "        entropy = entropy - yesLabel * log((yesLabel),2)\n",
    "    if noLabel != 0:\n",
    "        entropy = entropy - noLabel * log((noLabel),2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    "# Where to ask for parent enthropy 1-\n",
    "    best_info_gain = 0.0\n",
    "    best_thresh = None\n",
    "    aux_gain = 0\n",
    "    parent_gain = 0\n",
    "    lenData = len(data)\n",
    "    yesLabel = 0\n",
    "    noLabel = 0\n",
    "    first = True\n",
    "    featureMap = []\n",
    "    mixed = 0    \n",
    "    parent_gain = calc_entropy(data)\n",
    "    aux_data = []\n",
    "#     for continuous variables\n",
    "    data.sort(key=lambda x: x.features[feature_idx])\n",
    "    for i in range(0,lenData-1):\n",
    "        if data[i].features[feature_idx] == data[i+1].features[feature_idx]:\n",
    "            if data[i].label == 0:\n",
    "                yesLabel = yesLabel + 1\n",
    "            else:\n",
    "                noLabel = noLabel + 1\n",
    "        if data[i].features[feature_idx] != data[i+1].features[feature_idx]:\n",
    "            if data[i+1].label == 0:\n",
    "                yesLabel = yesLabel + 1\n",
    "            else:\n",
    "                noLabel = noLabel + 1\n",
    "            aux_data = data[i+1]\n",
    "            if yesLabel > 0 and noLabel > 0:\n",
    "                featureMap.append([data[i],0.5])\n",
    "            else:\n",
    "                featureMap.append([data[i],data[i].label])\n",
    "            yesLabel = 0\n",
    "            noLabel = 0\n",
    "    if data[lenData-2].features[feature_idx]!=data[lenData-1].features[feature_idx]:\n",
    "        featureMap.append([data[lenData-1],0])\n",
    "    else:\n",
    "        if yesLabel > 0 and noLabel > 0:\n",
    "            featureMap.append([aux_data,0.5])\n",
    "        else:\n",
    "            featureMap.append([aux_data,0.5])\n",
    "            \n",
    "    lenMap = len(featureMap)\n",
    "    for i in range(0,lenMap-1):\n",
    "        # Calculate entropy\n",
    "        threshold = (featureMap[i][0].features[feature_idx] + featureMap[i+1][0].features[feature_idx])/2\n",
    "        left_split, right_split = split_dataset(data, feature_idx, threshold)\n",
    "        print(\"Left entropy\")\n",
    "        print(calc_entropy(left_split))\n",
    "        print(\"Right entropy\")\n",
    "        print(calc_entropy(right_split))\n",
    "        aux_gain = parent_gain - (len(left_split)/lenData)*calc_entropy(left_split) - (len(right_split)/lenData)*calc_entropy(right_split)\n",
    "        if aux_gain > best_info_gain:\n",
    "            best_info_gain = aux_gain\n",
    "            best_thresh = threshold\n",
    "    \n",
    "    return (best_info_gain, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "    aux_best_thresh = 0\n",
    "    best_info_gain = 0\n",
    "    aux_best_info_gain = 0\n",
    "    for i in range(0,len(data[0].features)):\n",
    "        aux_best_info_gain, aux_best_thresh = calc_best_threshold(data,i)\n",
    "        if aux_best_info_gain >= best_info_gain:\n",
    "            best_thresh = aux_best_thresh\n",
    "            best_feature = i\n",
    "            best_info_gain = aux_best_info_gain\n",
    "    return (best_feature, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLeafNode(data):\n",
    "    noLabel = 0\n",
    "    yesLabel = 0\n",
    "    node = TreeNode()\n",
    "    node.is_leaf = True\n",
    "    # check labels in data\n",
    "    for dataPoint in data:\n",
    "        if dataPoint.label == 0:\n",
    "            noLabel = noLabel + 1\n",
    "        else:\n",
    "            yesLabel = yesLabel + 1\n",
    "    # decide prediction\n",
    "    if yesLabel == noLabel:\n",
    "        node.prediction = 0.5\n",
    "    elif yesLabel > noLabel:\n",
    "        node.prediction = 1\n",
    "    else:\n",
    "        node.prediction = 0\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDecisionTree(data, max_levels):\n",
    "    if max_levels == 1 or calc_entropy(data)== 0:\n",
    "        return createLeafNode(data)\n",
    "    node = TreeNode()\n",
    "    node.is_leaf = False\n",
    "    best_feature, best_thresh = identify_best_split(data)\n",
    "    # keep track of previous index evaluated\n",
    "    node.feature_idx = best_feature\n",
    "    node.thresh_val = best_thresh\n",
    "    left_split, right_split = split_dataset(data, best_feature, best_thresh)\n",
    "    node.left_child = createDecisionTree(left_split,max_levels-1)\n",
    "    node.right_child = createDecisionTree(right_split,max_levels-1)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Given a test set, the function `calcAccuracy` returns the accuracy of the classifier. You'll use the `makePrediction` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAccuracy(tree_root, data):\n",
    "    fold = 5\n",
    "    sizeStep = floor(len(data)/fold)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    finalStep = sizeStep\n",
    "    sumIncorrect = 0\n",
    "    errorAverage = 0\n",
    "    while i < fold:\n",
    "        while j < finalStep:\n",
    "            if data[j].label != make_prediction(tree_root,data[j]):\n",
    "                sumIncorrect = sumIncorrect + 1\n",
    "            j = j + 1\n",
    "        if i == fold-1:\n",
    "            finalStep = len(data)\n",
    "        else:\n",
    "            finalStep = finalStep + sizeStep\n",
    "        errorAverage = errorAverage + sumIncorrect/sizeStep\n",
    "        sumIncorrect = 0\n",
    "        i = i + 1\n",
    "    return errorAverage/fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy\n",
      "0.9852281360342516\n",
      "X1\n",
      "Left entropy\n",
      "0.961236604722876\n",
      "Right entropy\n",
      "0.5435644431995964\n",
      "(0.18310473570119642, 0.5)\n",
      "X2\n",
      "Left entropy\n",
      "0.9940302114769565\n",
      "Right entropy\n",
      "0.8812908992306927\n",
      "(0.04488331134123025, 0.5)\n",
      "Left entropy\n",
      "0.961236604722876\n",
      "Right entropy\n",
      "0.5435644431995964\n",
      "Left entropy\n",
      "0.9940302114769565\n",
      "Right entropy\n",
      "0.8812908992306927\n",
      "(0, 0.5)\n"
     ]
    }
   ],
   "source": [
    "# edit the code here - this is just a sample to get you started\n",
    "import time\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "\n",
    "train_set = d[0:300]\n",
    "test_set = d[301:360]\n",
    "\n",
    "# create the decision tree\n",
    "start = time.time()\n",
    "tree = createDecisionTree(train_set, 10)\n",
    "\n",
    "end = time.time()\n",
    "print ('Time taken:', end - start)\n",
    "\n",
    "# calculate the accuracy of the tree\n",
    "accuracy = calcAccuracy(tree, test_set)\n",
    "print ('The accuracy on the test set is ', str(accuracy * 100.0))\n",
    "tree.printTree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
